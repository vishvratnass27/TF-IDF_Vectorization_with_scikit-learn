{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db6524da",
   "metadata": {},
   "source": [
    "# TfidfVectorizer\n",
    "TfidfVectorizer is a class provided by scikit-learn (a popular machine learning library in Python) for converting a collection of raw documents (text) into a matrix of TF-IDF features. TF-IDF stands for Term Frequency-Inverse Document Frequency, and it is a numerical statistic used in natural language processing and information retrieval to represent the importance of a term within a document relative to a collection of documents (corpus)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90743979",
   "metadata": {},
   "source": [
    "# TfidfVectorizer():\n",
    "This is the constructor for creating a TfidfVectorizer object. You can customize the vectorizer's behavior by passing various parameters as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f082cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bdd378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'LW awarded best innovative training provider by linkedin',\n",
    "    'best awarded serach engine provider is google',\n",
    "    'we are LW working for making india future ready',\n",
    "    'if we want to search in india are go to google',\n",
    "    'i am the best and how is the best'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b179a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3575cf43",
   "metadata": {},
   "source": [
    "# fit(raw_documents):\n",
    "This function analyzes the text data provided in raw_documents and builds the vocabulary and IDF (Inverse Document Frequency) parameters needed for TF-IDF vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d41c902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1317044",
   "metadata": {},
   "source": [
    "# transform(raw_documents): \n",
    "This function transforms the input text data into a TF-IDF matrix based on the vocabulary and IDF parameters learned from the fit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faf2bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93710d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x30 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 40 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fa0fda",
   "metadata": {},
   "source": [
    "# fit_transform(raw_documents): \n",
    "This is a combination of the fit and transform methods, which is commonly used to both fit the vectorizer to the data and transform it in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f10d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_output = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9155819a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x30 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 40 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ca4b5",
   "metadata": {},
   "source": [
    "# get_feature_names_out():\n",
    "Returns an array of feature names (terms) in the order they appear in the TF-IDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17163610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['am', 'and', 'are', 'awarded', 'best', 'by', 'engine', 'for',\n",
       "       'future', 'go', 'google', 'how', 'if', 'in', 'india', 'innovative',\n",
       "       'is', 'linkedin', 'lw', 'making', 'provider', 'ready', 'search',\n",
       "       'serach', 'the', 'to', 'training', 'want', 'we', 'working'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc7000",
   "metadata": {},
   "source": [
    "# get_params():\n",
    "Returns a dictionary of the parameters that were set when creating the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45fd02b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = vectorizer.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92e97baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.float64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'norm': 'l2',\n",
       " 'preprocessor': None,\n",
       " 'smooth_idf': True,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'sublinear_tf': False,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'use_idf': True,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ead1aa",
   "metadata": {},
   "source": [
    "# vocabulary_:\n",
    "\n",
    "This attribute contains the vocabulary (unique terms) learned from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d27f66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lw': 18,\n",
       " 'awarded': 3,\n",
       " 'best': 4,\n",
       " 'innovative': 15,\n",
       " 'training': 26,\n",
       " 'provider': 20,\n",
       " 'by': 5,\n",
       " 'linkedin': 17,\n",
       " 'serach': 23,\n",
       " 'engine': 6,\n",
       " 'is': 16,\n",
       " 'google': 10,\n",
       " 'we': 28,\n",
       " 'are': 2,\n",
       " 'working': 29,\n",
       " 'for': 7,\n",
       " 'making': 19,\n",
       " 'india': 14,\n",
       " 'future': 8,\n",
       " 'ready': 21,\n",
       " 'if': 12,\n",
       " 'want': 27,\n",
       " 'to': 25,\n",
       " 'search': 22,\n",
       " 'in': 13,\n",
       " 'go': 9,\n",
       " 'am': 0,\n",
       " 'the': 24,\n",
       " 'and': 1,\n",
       " 'how': 11}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9740f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get(\"lw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "597ea1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_['best']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36c0b7",
   "metadata": {},
   "source": [
    "# idf_: \n",
    "An attribute that contains the inverse document frequency (IDF) of each term in the vocabulary after the vectorizer has been fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f407a2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.09861229, 2.09861229, 1.69314718, 1.69314718, 1.40546511,\n",
       "       2.09861229, 2.09861229, 2.09861229, 2.09861229, 2.09861229,\n",
       "       1.69314718, 2.09861229, 2.09861229, 2.09861229, 1.69314718,\n",
       "       2.09861229, 1.69314718, 2.09861229, 1.69314718, 2.09861229,\n",
       "       1.69314718, 2.09861229, 2.09861229, 2.09861229, 2.09861229,\n",
       "       2.09861229, 2.09861229, 2.09861229, 1.69314718, 2.09861229])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53a4c0",
   "metadata": {},
   "source": [
    "# dtype:\n",
    "'vectorizer.dtype' returns the data type of the TF-IDF matrix created by the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dc0e754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a5ff60",
   "metadata": {},
   "source": [
    "# fixed_vocabulary_:\n",
    "`vectorizer.fixed_vocabulary_` indicates whether a fixed vocabulary is used during TF-IDF vectorization (True if fixed, False otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c01ff1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fixed_vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff5e3b",
   "metadata": {},
   "source": [
    "# vectorizer.lowercase:\n",
    "`vectorizer.lowercase` specifies whether the vectorizer should convert text to lowercase during tokenization (True if it does, False if it doesn't)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "412c3413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee9312",
   "metadata": {},
   "source": [
    "# toarray():\n",
    "`transform_output.toarray()` converts the sparse matrix `transform_output` to a dense NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4150701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.31888178, 0.26470068,\n",
       "        0.39524574, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.39524574, 0.        , 0.39524574, 0.31888178, 0.        ,\n",
       "        0.31888178, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.39524574, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.35894109, 0.29795353,\n",
       "        0.        , 0.44489823, 0.        , 0.        , 0.        ,\n",
       "        0.35894109, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.35894109, 0.        , 0.        , 0.        ,\n",
       "        0.35894109, 0.        , 0.        , 0.44489823, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.29258431, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.36265071, 0.36265071, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.29258431,\n",
       "        0.        , 0.        , 0.        , 0.29258431, 0.36265071,\n",
       "        0.        , 0.36265071, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.29258431, 0.36265071],\n",
       "       [0.        , 0.        , 0.23684538, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.29356375,\n",
       "        0.23684538, 0.        , 0.29356375, 0.29356375, 0.23684538,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.29356375, 0.        , 0.        ,\n",
       "        0.58712749, 0.        , 0.29356375, 0.23684538, 0.        ],\n",
       "       [0.32538662, 0.32538662, 0.        , 0.        , 0.43583042,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.32538662, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.26251987, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.65077324,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_output.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa72658",
   "metadata": {},
   "source": [
    "# build_analyzer()\n",
    "\n",
    "This method returns a callable to process input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5df55ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'an', 'example', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "analyzer = vectorizer.build_analyzer()\n",
    "text = \"This is an example sentence.\"\n",
    "tokens = analyzer(text)\n",
    "print(tokens)  # Output: ['this', 'is', 'an', 'example', 'sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb25ef",
   "metadata": {},
   "source": [
    "# build_preprocessor()\n",
    "\n",
    "This method returns a function to preprocess the text before tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e4bb789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am the best\n"
     ]
    }
   ],
   "source": [
    "preprocessor = vectorizer.build_preprocessor()\n",
    "text = \"i am the best.\"\n",
    "preprocessed_text = preprocessor(\"i am the best\")\n",
    "print(preprocessed_text)  # Output: 'this is an example sentence'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa9651",
   "metadata": {},
   "source": [
    "# build_tokenizer()\n",
    "\n",
    "This method returns a function that splits a string into a sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "172c325b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = vectorizer.build_tokenizer()\n",
    "text = \"This is an example sentence.\"\n",
    "tokens = tokenizer(text)\n",
    "print(tokens)  # Output: ['This', 'is', 'an', 'example', 'sentence', '.']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a8e56f",
   "metadata": {},
   "source": [
    "# decode(doc)\n",
    "\n",
    "This method decodes the input into a string of Unicode symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb75456b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 26)\t0.39524574252810757\n",
      "  (0, 20)\t0.31888177640211135\n",
      "  (0, 18)\t0.31888177640211135\n",
      "  (0, 17)\t0.39524574252810757\n",
      "  (0, 15)\t0.39524574252810757\n",
      "  (0, 5)\t0.39524574252810757\n",
      "  (0, 4)\t0.26470068018333703\n",
      "  (0, 3)\t0.31888177640211135\n"
     ]
    }
   ],
   "source": [
    "encoded_text = vectorizer.transform(corpus)\n",
    "decoded_text = vectorizer.decode(encoded_text[0])\n",
    "print(decoded_text)  # Output: 'this is encoded text'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a8ce98",
   "metadata": {},
   "source": [
    "# get_params([deep])\n",
    "\n",
    "This method gets parameters for this estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1a1f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = vectorizer.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "532b47ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.float64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'norm': 'l2',\n",
       " 'preprocessor': None,\n",
       " 'smooth_idf': True,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'sublinear_tf': False,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'use_idf': True,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40540af8",
   "metadata": {},
   "source": [
    "# get_stop_words()\n",
    "\n",
    "This method builds or fetches the effective stop words list.\n",
    "\n",
    "stop_words (default=None): You can specify a list of stop words\n",
    "    (common words like \"the,\" \"and,\" \"is\" that are often removed from text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f76d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = vectorizer.get_stop_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98219d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb52a5",
   "metadata": {},
   "source": [
    "# inverse_transform(X)\n",
    "\n",
    "This method returns terms per document with nonzero entries in X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8596b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_per_document = vectorizer.inverse_transform(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55727ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['training', 'provider', 'lw', 'linkedin', 'innovative', 'by',\n",
       "        'best', 'awarded'], dtype='<U10'),\n",
       " array(['serach', 'provider', 'is', 'google', 'engine', 'best', 'awarded'],\n",
       "       dtype='<U10'),\n",
       " array(['working', 'we', 'ready', 'making', 'lw', 'india', 'future', 'for',\n",
       "        'are'], dtype='<U10'),\n",
       " array(['we', 'want', 'to', 'search', 'india', 'in', 'if', 'google', 'go',\n",
       "        'are'], dtype='<U10'),\n",
       " array(['the', 'is', 'how', 'best', 'and', 'am'], dtype='<U10')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_per_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab1b068",
   "metadata": {},
   "source": [
    "# **set_params(params)\n",
    "\n",
    "This method sets the parameters of this estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d55a56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words='english')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.set_params(stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70651d1",
   "metadata": {},
   "source": [
    "# TfidfVectorizer()\n",
    "\n",
    "This is the constructor function for creating a TfidfVectorizer object.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "stop_words (default=None): You can specify a list of stop words (common words like \"the,\" \"and,\" \"is\" that are often removed from text).\n",
    "\n",
    "max_df (default=1.0): Ignore terms that have a document frequency higher than this threshold.\n",
    "\n",
    "min_df (default=1): Ignore terms that have a document frequency lower than this threshold.\n",
    "\n",
    "max_features (default=None): Limit the number of features (words) to this maximum number based on term frequency.\n",
    "\n",
    "ngram_range (default=(1, 1)): Specify the range of n-grams to consider (e.g., (1, 2) for unigrams and bigrams).\n",
    "\n",
    "use_idf (default=True): Enable inverse-document-frequency reweighting.\n",
    "\n",
    "smooth_idf (default=True): Add 1 to document frequencies to avoid division by zero.\n",
    "\n",
    "sublinear_tf (default=False): Apply sublinear scaling to the term frequency.\n",
    "\n",
    "token_pattern (default=r\"(?u)\\b\\w\\w+\\b\"): Regular expression pattern for tokenization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
